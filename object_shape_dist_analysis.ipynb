{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import utils\n",
    "import rdd_data_loader\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import rpn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "import torchvision.transforms.v2 as T_v2\n",
    "from torchvision.transforms.v2 import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\citak\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# DinoV2 transformation\n",
    "\n",
    "# Create a proper transform for object detection that transforms both images and targets\n",
    "def get_transform():\n",
    "    transforms = []\n",
    "    \n",
    "    # Add spatial transforms that will update bounding boxes accordingly\n",
    "    transforms.append(T_v2.Resize(512))\n",
    "    transforms.append(T_v2.CenterCrop(448))\n",
    "    \n",
    "    # Convert to tensors and normalize\n",
    "    transforms.append(T_v2.ToTensor())\n",
    "    transforms.append(T_v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    \n",
    "    return T_v2.Compose(transforms)\n",
    "\n",
    "\n",
    "dino_transform = get_transform()\n",
    "\n",
    "train_dataset = rdd_data_loader.RoadDamageDataset(csv_file='train_paths.csv', transforms=dino_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=rdd_data_loader.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/1 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[264., 600., 312., 672.],\n",
      "        [345., 514., 379., 561.]], device='cuda:0'), 'labels': tensor([5, 5], device='cuda:0')}, {'boxes': tensor([[147., 188., 267., 327.]], device='cuda:0'), 'labels': tensor([2], device='cuda:0')}, {'boxes': tensor([[ 74., 325., 111., 591.]], device='cuda:0'), 'labels': tensor([5], device='cuda:0')}, {'boxes': tensor([[442., 527., 562., 552.],\n",
      "        [ 42., 459., 251., 493.],\n",
      "        [496., 611., 640., 640.]], device='cuda:0'), 'labels': tensor([1, 1, 1], device='cuda:0')}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(enumerate(train_dataloader), total=1, desc=f\"Epoch {10+1}\")\n",
    "\n",
    "for batch_idx, (images, targets) in pbar:\n",
    "    images = [img.to(device) for img in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    print(targets)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original boxes: BoundingBoxes([[554, 720, 578, 740]], format=BoundingBoxFormat.XYXY, canvas_size=(1125, 2000))\n",
      "Original image size: (2000, 1125)\n",
      "Transformed boxes: BoundingBoxes([[ 21, 295,  31, 304]], format=BoundingBoxFormat.XYXY, canvas_size=(448, 448))\n",
      "Transformed image shape: torch.Size([3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load image as PIL Image first\n",
    "path = \"sample_scene_city.jpg\"\n",
    "img_pil = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "# Get image dimensions for BoundingBoxes\n",
    "img_width, img_height = img_pil.size  # Get dimensions from PIL Image\n",
    "\n",
    "# Create BoundingBoxes with correct canvas size (height, width)\n",
    "boxes = tv_tensors.BoundingBoxes(\n",
    "    [[554, 720, 578, 740]],\n",
    "    format=\"XYXY\", \n",
    "    canvas_size=(img_height, img_width)  # Pass (H, W) not the array shape\n",
    ")\n",
    "\n",
    "# Define transforms\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize(512),\n",
    "    v2.CenterCrop(448),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Original boxes:\", boxes)\n",
    "print(\"Original image size:\", img_pil.size)  # PIL image size is (W, H)\n",
    "\n",
    "# Apply transforms\n",
    "out_img, out_boxes = transforms(img_pil, boxes)\n",
    "\n",
    "print(\"Transformed boxes:\", out_boxes)\n",
    "print(\"Transformed image shape:\", out_img.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
